{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SunLife_Vector Institute Workshop (Dec 2021).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayyed-uoft/sunlife/blob/main/SunLife_Vector_Institute_Workshop_(Dec_2021).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwooQe1JN-v8"
      },
      "source": [
        "# Vector Institute + Sun Life Financial\n",
        "## Fundamentals of Random Forests\n",
        "\n",
        "Welcome to ‘Fundamentals of Random Forests’ by Vector Institute!\n",
        "This is a Python tutorial in the ‘Fundamentals of Random Forests’ 2-day workshop. \n",
        "\n",
        "This program was developed for Sun Life Financial to give a mostly technical audience the opportunity to practice with Decision Tree and Random Forest models using the 'sklearn' Python package with a real and relevant dataset.\n",
        "\n",
        "Instructor: Sayyed Nezhadi | Assignment Developer: Sayyed Nezhadi | Course Director: Shingai Manjengwa (@Tjido)\n",
        "Never stop learning!\n",
        "\n",
        "### Assignment: Regression using Decision Tree and Random Forest models\n",
        "In this assignment you are going to learn how to process data, build and train Decition Tree and Random Forest models to predict the cost, and hence severity, of insurance claims. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnBLWbPbW9Fj"
      },
      "source": [
        "## Data: Loading and Analysis\n",
        "In this part of the code we will load the data, analyze it, and visualize it.\n",
        "\n",
        "We are going to use a public dataset in [Kaggle](https://www.kaggle.com/c/allstate-claims-severity) from Allstate insurance company in USA. They are  currently developing automated methods of predicting cost, and hence severity, of claims. Each row in this dataset represents an insurance claim. You must predict the value for the 'loss' column. Variables prefaced with 'cat' are categorical, while those prefaced with 'cont' are continuous. \n",
        "\n",
        "There are 116 categorical variables and 14 continuous (real) variables. All the column names and categorical values are annonomized for privacy reasons.  \n",
        "\n",
        "Data is provided in two splits of \"train\" and \"test\". For this lesson, we will only load the \"train\" dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKT7y0KQIReH"
      },
      "source": [
        "### Initializing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YHiQW4IL4z7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx-FTPovnE_K"
      },
      "source": [
        "### Loading data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWdxR0VqoLfe"
      },
      "source": [
        "Loading the training dataset from a Zip file online using \"pandas\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLclSzLZMMoY"
      },
      "source": [
        "data = pd.read_csv('https://github.com/sayyed-uoft/sunlife/raw/main/Allstate_Claims_Severity.zip', compression='zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwLAMos-IXt2"
      },
      "source": [
        "### Analyzing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToVPMDY7oif8"
      },
      "source": [
        "Let's take a quick look at the data. It is clearly annonomized. We won't be able to use any subject matter expertise to help with the feature engineering. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfHD31_1w0s6"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVBOVfRGo16S"
      },
      "source": [
        "# Getting overall information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i7Kd_j23jVP"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioxBo-Bzo7Bb"
      },
      "source": [
        "Checking for missing information - fortunately, the data is already clean.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17YG-NPk7NQ2"
      },
      "source": [
        "# number of missing data by column\n",
        "data.isnull().sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-krpIgon8ExL"
      },
      "source": [
        "# Is ther any non-zero in that list?\n",
        "data.isnull().sum(axis=0).any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YOuNFXXpX5C"
      },
      "source": [
        "Let's look at the distribution of numerical/continous variables. Looks like they are already normalized to (0, 1) range. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R464rpG8bjH"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdKa7RtGqDye"
      },
      "source": [
        "Let's check how they are distributed. We could plot the distributions, but we skip that for this assignment. We will rather just look how skewed they are. The result shows all the columns are fairly symmetric except \"loss\", that is the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bqbtbvh9U13"
      },
      "source": [
        "data.skew()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dtN6NoGqnkJ"
      },
      "source": [
        "Now, we can look at the distribution of the \"loss\"\" variable using a Violin plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7BUGFxa9kqY"
      },
      "source": [
        "sns.violinplot(y='loss', data=data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is very skewed and the range of numbers is very high too."
      ],
      "metadata": {
        "id": "idkxvFi5e-ew"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtmNUNoitlhH"
      },
      "source": [
        "### Pre-process data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQfEH1VXt8aZ"
      },
      "source": [
        "Let's first convert \"id\" to an index as this is not a feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "957hhnE3uPeT"
      },
      "source": [
        "data.set_index('id', inplace=True)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpLYB3p-vuv9"
      },
      "source": [
        "**We** saw that the \"loss\" values are very skewed and there is a very large range of numbers. Let's convert it to logarithmic scale. We also had some noisy/very small data (e.g. loss = 0.67). Therefore, it is better to use log(1+x)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWIVjs3t_g0B"
      },
      "source": [
        "data.loss = np.log1p(data.loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKw_C5Nn_qCf"
      },
      "source": [
        "Let's look at it again. This looks more symmetric. It will be better to use this as the response variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLOCYprg_pOp"
      },
      "source": [
        "sns.violinplot(y='loss', data=data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4n62BkurlcZ"
      },
      "source": [
        "The \"skew\" metric is dropped significantly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eg54fma_xpx"
      },
      "source": [
        "data.loss.skew()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Iv7bfzH_4-h"
      },
      "source": [
        "\n",
        "Now, we need to encode categorical columns into one-hot vectors. Let's first look at unique values for each column (only first 116 columns that are categorical)\n",
        "You can learn more about one-hot encoding >> (https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jemF6jS_4UV"
      },
      "source": [
        "data.iloc[:, :116].nunique().value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z35PMINeCLwP"
      },
      "source": [
        "Looks like most of the columns have 2 unique values. So, we won't create a huge amount of columns We can convert to one-hot / dummy variables now (only \"0\"s and \"1\"s). We will choose to drop the first one to eliminate redundant data. For eaxmple, the columns with only two unique values will be converted to only one column. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa3S1fyXCLAQ"
      },
      "source": [
        "data = pd.get_dummies(data, drop_first=True)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulxacpbOETRy"
      },
      "source": [
        "Now we have 1038 columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55IpxyTLDB7V"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zznztN1Z3thD"
      },
      "source": [
        "Now, we need to do the following to be ready to train a model using \"sklearn\":\n",
        "\n",
        "- Separate the features from labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AfCGNae3sVf"
      },
      "source": [
        "features = data.drop(['loss'], axis=1)\n",
        "labels = data['loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmlBG6g_xjWQ"
      },
      "source": [
        "\n",
        "- We need to split it to training and test (validation) sets for model evaluation. We keep 80% for training and 20% for test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4X8FxnKxyJG"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGHccdoQyMai"
      },
      "source": [
        "print(\"Number of training samples:\", labels_train.shape[0])\n",
        "print(\"Number of testing samples:\", labels_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbk4F0IIEZB5"
      },
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "We have prepared our data and are ready to train a model.\n",
        "\n",
        "We will compare the following two models:\n",
        "\n",
        "- Decistion Tree\n",
        "- Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK9gKAHGEvFc"
      },
      "source": [
        "### Decision Tree:\n",
        "\n",
        "Let's instantiate a Decision Tree model and train (fit) it with the training data. For now, we choose the default parameters without any restriction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caaZ9CgVYD3I"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Instantiate the model \n",
        "model = DecisionTreeRegressor(random_state=0)\n",
        "# Train the model\n",
        "model.fit(features_train, labels_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXh28ZHVFmhd"
      },
      "source": [
        "Below are the parameters used for this decision tree (default parameters):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3g-ZvRsFwQ8"
      },
      "source": [
        "model.get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czJv2Le-F2I4"
      },
      "source": [
        "Now, we can use the trained model to predict the response variable for test samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pffrnVvfGYJj"
      },
      "source": [
        "# Predict the test labels\n",
        "preds = model.predict(features_test)\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Op8Ag4PGGIC"
      },
      "source": [
        "To see how our model performs, we can use a Regression metric. One popular metric is MAE (Mean Absolute Error). Don't forget our model predicts the logarithm of \"loss\" (log(1+x)). We need to reverse it first using \"expm1\" function: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Monkl_tqGyMV"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Calculate MAE for test data\n",
        "mean_absolute_error(np.expm1(labels_test), np.expm1(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JmGvgpWHJDr"
      },
      "source": [
        "Let's see what the error is for training data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EqKcisrST9U"
      },
      "source": [
        "# Calculate MAE for train data \n",
        "mean_absolute_error(np.expm1(labels_train), np.expm1(model.predict(features_train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tQYtntgHb4-"
      },
      "source": [
        "Wow! That's almost zero! That means a perfect fit. \n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl-sKQZGIVXn"
      },
      "source": [
        "\n",
        "> **Question:**\n",
        "> The error on the training data is very low but on the testing data is high. What is this sign of? Please explain.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "699wAohjI8US"
      },
      "source": [
        "The constructed tree is going to be very big and very deep. Let's limit the size of the tree by limiting it's depth to 3: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK8hDw1aROwh"
      },
      "source": [
        "# Create a new model with limited depth\n",
        "model = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
        "# Train \n",
        "model.fit(features_train, labels_train)\n",
        "# Predict test labels\n",
        "preds = model.predict(features_test)\n",
        "# MAE for test data\n",
        "mean_absolute_error(np.expm1(labels_test), np.expm1(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTuTtlTmKgkX"
      },
      "source": [
        "Interestingly, the error was reduced even when we limited the tree. Let's check the error on training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB5PGDzjKzSE"
      },
      "source": [
        "# Calculate MAE for train data \n",
        "mean_absolute_error(np.expm1(labels_train), np.expm1(model.predict(features_train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJPALJt4LD5i"
      },
      "source": [
        "> **Question:**\n",
        "> The error on the test data is lower and the error on training data is comparable to that. Please explain why that happened? Is that good?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERIXP-STLpLN"
      },
      "source": [
        "Let's plot this tree and look at the conditions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaGig7fCHU2n"
      },
      "source": [
        "from sklearn import tree\n",
        "\n",
        "plt.figure(figsize=(30, 20))\n",
        "tree.plot_tree(model)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks nice! :)"
      ],
      "metadata": {
        "id": "tTeOYip6rYwa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnZUtO1DL11X"
      },
      "source": [
        "> **Task:** Try different tree depths and see if you can get even better results. Report the depth and the corresponding MAE. You can use Grid Search function from \"sklearn\" or you can try it manually  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYEvSfdNMzB8"
      },
      "source": [
        "### Random Forest:\n",
        "\n",
        "Now, we will use a Random Forest model and train (fit) it with the training data. For now, we choose the default parameters with 50 estimators. \n",
        "\n",
        "**Note:** This may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxD7yUXBqf3P"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create the model\n",
        "model = RandomForestRegressor(n_estimators=50, random_state=0)\n",
        "# Train\n",
        "model.fit(features_train, labels_train.values)\n",
        "# Predict test labels\n",
        "preds = model.predict(features_test)\n",
        "# MAE \n",
        "mean_absolute_error(np.expm1(labels_test), np.expm1(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM2_wIFwQQft"
      },
      "source": [
        "We got a much better result! \n",
        "\n",
        "Let's look at MAE for training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqNf3xjhQpiG"
      },
      "source": [
        "# Calculate MAE for train data \n",
        "mean_absolute_error(np.expm1(labels_train), np.expm1(model.predict(features_train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLlkoxznQwPi"
      },
      "source": [
        "It's not zero but much lower than the test error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGIqE_R_RnJt"
      },
      "source": [
        "> **Task:** Play with different parameters and see if you can get a better result while avoiding overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryuKCno1SM6E"
      },
      "source": [
        "Below are the paramaers we used for our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NROOaSUA00LT"
      },
      "source": [
        "model.get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CESsTKo1SZtI"
      },
      "source": [
        "One great feature of Random Forest is that it will give you the importance of the features. Thsi is great for vfeature engineering and to speed up the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21vxPiACcMmV"
      },
      "source": [
        "# Importance scores sorted from high to low\n",
        "np.sort(model.feature_importances_)[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKHZX2yPdb2o"
      },
      "source": [
        "# Indices of top 10 important features\n",
        "indices = np.argsort(model.feature_importances_)[-10:][::-1]\n",
        "indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVVtgtZAcxEs"
      },
      "source": [
        "# names of the top 10 important features (sorted)\n",
        "cols = features.columns[indices]\n",
        "cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cS8XQoFTQyC"
      },
      "source": [
        "We can plot the top 10 importance scores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw5Zwt2zTWjA"
      },
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(x=cols, height=np.sort(model.feature_importances_)[-10:][::-1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfRF8IfsXHED"
      },
      "source": [
        "> **Question & Task:** What are other ways to interpret the results of a Regression model? What other metrics or graphs would you suggest? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFw0b5BrZHS1"
      },
      "source": [
        "**Congratulations, you have completed a tutorial in the ‘Fundamentals of Random Forests’ program!**\n",
        "\n",
        "Vector Institute & Sun Life Financial | Course Director: Shingai Manjengwa (@Tjido) | Instructor: Sayyed Nezhadi  | Contact: learn@vectorinstitute.com\n",
        "Never stop learning!"
      ]
    }
  ]
}